{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection Statistics\n",
    "\n",
    "This notebook will calculate some collection statistic during the various phases of the processing. In particular: \n",
    "* after download phase (and download retry)\n",
    "* after automatic file recovering phase\n",
    "* after manual file recovering phase\n",
    "* after indexing phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download\n",
    "\n",
    "After collection download we want to know:\n",
    "* how many datasets are complete (all the files for the dataset are downloaded)\n",
    "* how many datasets are partial (not all the files for the dataset are downloaded)\n",
    "* how many datasets are empty (no files for the dataset are downloaded)\n",
    "* how many files have not a valid RDF extension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned: 1000\n",
      "Scanned: 2000\n",
      "Scanned: 3000\n",
      "Scanned: 4000\n",
      "Scanned: 5000\n",
      "Scanned: 6000\n",
      "Scanned: 7000\n",
      "Scanned: 8000\n",
      "Scanned: 9000\n",
      "Scanned: 10000\n",
      "Scanned: 11000\n",
      "Scanned: 12000\n",
      "Scanned: 13000\n",
      "Scanned: 14000\n",
      "Scanned: 15000\n",
      "Scanned: 16000\n",
      "Scanned: 17000\n",
      "Scanned: 18000\n",
      "Scanned: 19000\n",
      "Scanned: 20000\n",
      "Scanned: 21000\n",
      "Scanned: 22000\n",
      "Scanned: 23000\n",
      "Scanned: 24000\n",
      "Scanned: 25000\n",
      "Scanned: 26000\n",
      "Scanned: 27000\n",
      "Scanned: 28000\n",
      "Scanned: 29000\n",
      "Scanned: 30000\n",
      "Scanned: 31000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@param datasets_directory_path path to the directory where there are all the datasets\n",
    "@param checker_error_log_path path of the log file of the dataset checker\n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postDownloadStats():\n",
    "\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "    f_log_checker = open(checker_error_log_path, \"r\")\n",
    "\n",
    "    n_datasets = 0\n",
    "    n_empty = 0\n",
    "    n_full = 0\n",
    "\n",
    "    #scan the datasets folders and count how many datasets are full, partial and empty\n",
    "\n",
    "    for folder in os.scandir(datasets_directory_path):\n",
    "        \n",
    "        n_datasets+=1\n",
    "\n",
    "        if n_datasets % 1000 == 0:\n",
    "            print(\"Scanned: \"+str(n_datasets))\n",
    "\n",
    "        dataset_json_path = datasets_directory_path+\"/\"+folder.name+\"/dataset_metadata.json\"\n",
    "\n",
    "        #open the dataset.json file \n",
    "        dataset_json_file=open(dataset_json_path, \"r\")\n",
    "        \n",
    "        #load the json object present in the json datasets list\n",
    "        dataset_json = json.load(dataset_json_file,strict=False)\n",
    "\n",
    "        if dataset_json[\"download_info\"][\"downloaded\"] == 0:\n",
    "            n_empty+=1 \n",
    "        \n",
    "        if dataset_json[\"download_info\"][\"downloaded\"] == dataset_json[\"download_info\"][\"total_URLS\"]:\n",
    "            n_full+=1\n",
    "        \n",
    "        dataset_json_file.close()\n",
    "        del(dataset_json)\n",
    "        \n",
    "    #read the checker error_log\n",
    "\n",
    "    n_file = 0\n",
    "        \n",
    "    while True:\n",
    "\n",
    "        line = f_log_checker.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        #split the line\n",
    "        fields = line.split(\": \")\n",
    "\n",
    "        if fields[0] == \"File\":\n",
    "            n_file += 1\n",
    "\n",
    "    output_file.write(\"Number of datasets: \"+str(n_datasets)+\"\\n\")\n",
    "    output_file.write(\"Number of full datasets: \"+str(n_full)+\"\\n\")\n",
    "    output_file.write(\"Number of partial datasets: \"+str(n_datasets-n_full-n_empty)+\"\\n\")\n",
    "    output_file.write(\"Number of empty datasets: \"+str(n_empty)+\"\\n\")\n",
    "    output_file.write(\"Number of files that need to be assigned to an extension: \"+str(n_file)+\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    f_log_checker.close()\n",
    "\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/500GBHDD/Tesi/Datasets\"                                #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, '../../logs/statistics/post_download_statistics.txt')                 #path to the error log file\n",
    "\n",
    "checker_error_log_path = os.path.join(scriptDir, '../../logs/checker_error_log.txt')                  #path to the checker error log file\n",
    "postDownloadStats() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic File Recovering\n",
    "\n",
    "After the automatic file recovering we want to know: \n",
    "* how many files have been recovered\n",
    "* how many files need to be manually processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@param recover_error_log_path path to the file recover error log path \n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postAutomaticRecovering():\n",
    "    f_log_recover = open(recover_error_log_path, \"r\")\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "\n",
    "    #read the recover error_log\n",
    "\n",
    "    n_unrecovered_file = 0\n",
    "    n_recovered_file = 0\n",
    "        \n",
    "    while True:\n",
    "\n",
    "        line = f_log_recover.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        #split the line\n",
    "        fields = line.split(\": \")\n",
    "\n",
    "        if fields[0] == \"Dataset\":\n",
    "            n_unrecovered_file += 1\n",
    "        elif fields[0] == \"Recover in Dataset\": \n",
    "            n_recovered_file += 1\n",
    "\n",
    "    output_file.write(\"Number of recovered files: \"+str(n_recovered_file)+\"\\n\")\n",
    "    output_file.write(\"Number of unrecovered files: \"+str(n_unrecovered_file)+\"\\n\")\n",
    "\n",
    "    f_log_recover.close()\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/500GBHDD/Tesi/Datasets\"                                #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, '../../logs/statistics/post_automatic_recovering_statistics.txt')     #path to the error log file\n",
    "\n",
    "recover_error_log_path = os.path.join(scriptDir, '../../logs/recover_error_log.txt')                  #path to the checker error log file\n",
    "postAutomaticRecovering() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual File Recovering\n",
    "\n",
    "After the manual file recovering we want to know: \n",
    "* total number of files in the collection\n",
    "* total number of files with a no RDF extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIXES = [\".rdf\", \".rdfs\", \".ttl\", \".owl\", \".n3\", \".nt\", \".jsonld\", \".xml\", \".ntriples\", \".nq\", \".trig\", \".trix\"]\n",
    "\n",
    "'''\n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postManualRecovering():\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "\n",
    "    nFiles = 0\n",
    "    nNoRDFFiles = 0\n",
    "\n",
    "    for dataset in os.scandir(datasets_directory_path):\n",
    "        for file in os.scandir(dataset):\n",
    "            if file.name != \"dataset_metadata.json\" and file.name != \"dataset_content.json\":\n",
    "                file_suffix = pathlib.Path(file.path).suffix\n",
    "                if file_suffix not in SUFFIXES\n",
    "                    nNoRDFFiles += 1\n",
    "                nFiles+=1\n",
    "\n",
    "    output_file.write(\"Total number of files: \"+str(nFiles))\n",
    "    output_file.write(\"Total number of NO RDF files: \"+str(nNoRDFFiles))\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/500GBHDD/Tesi/Datasets\"                                #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, 'logs/post_manual_recovering_statistics.txt')     #path to the error log file\n",
    "\n",
    "postManualRecovering() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "After the indexing phase we want to know: \n",
    "* total number of mined files\n",
    "* total number of files that cannot be mined\n",
    "* total number of full datasets\n",
    "* total number of partial datasets\n",
    "* total number of empty datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postIndexing():\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "\n",
    "    full_datasets = 0\n",
    "    empty_datasets = 0\n",
    "    partial_dataset = 0\n",
    "    mined_files = 0\n",
    "\n",
    "    for dataset in os.scandir(datasets_directory_path):\n",
    "\n",
    "        #open the dataset metadata file\n",
    "        dataset_metadata_file = open(dataset.path+\"/dataset_metadata.json\", \"r\", encoding=\"utf-8\")\n",
    "        dataset_metadata = json.load(dataset_metadata_file, strict = False)\n",
    "        dataset_metadata_file.close()\n",
    "\n",
    "        mined_files += dataset_metadata[\"mined_files\"]\n",
    "\n",
    "        if dataset_metadata[\"mined_files\"] == 0:\n",
    "            empty_datasets += 1\n",
    "        elif dataset_metadata[\"mined_files\"] > 0 and dataset_metadata[\"mined_files\"] < (len(os.listdir(dataset))-2):\n",
    "            partial_dataset += 1\n",
    "        elif dataset_metadata[\"mined_files\"] == (len(os.listdir(dataset))-2):\n",
    "            full_datasets += 1 \n",
    "\n",
    "    output_file.write(\"Total number of mined files: \"+str(mined_files))\n",
    "    output_file.write(\"Total number of full datasets: \"+str(full_datasets) )\n",
    "    output_file.write(\"Total number of partial datasets: \"+str(partial_dataset) )\n",
    "    output_file.write(\"Total number of empty datasets: \"+str(empty_datasets) )\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/500GBHDD/Tesi/Datasets\"                                #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, 'logs/post_indexing_statistics.txt')     #path to the error log file\n",
    "\n",
    "postIndexing() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
