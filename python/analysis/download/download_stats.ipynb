{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Stats\n",
    "\n",
    "In this notebook we are going to retrieve some insights after the download phase. All the notebook output will be saved in the output directory.\n",
    "\n",
    "After the collection download we want to know:\n",
    "* how many links we tried to download\n",
    "* how many of these links are downloaded with success \n",
    "* how many of these links gave error\n",
    "* how many datasets are complete (all the files for the dataset are downloaded)\n",
    "* how many datasets are partial (not all the files for the dataset are downloaded)\n",
    "* how many datasets are empty (no files for the dataset are downloaded)\n",
    "* how many files have not a valid RDF extension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import tqdm \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "SUFFIXES = [\".rdf\", \".rdfs\", \".ttl\", \".owl\", \".n3\", \".nt\", \".jsonld\", \".xml\", \".ntriples\", \".nq\", \".trig\", \".trix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned: 1000\n",
      "Scanned: 2000\n",
      "Scanned: 3000\n",
      "Scanned: 4000\n",
      "Scanned: 5000\n",
      "Scanned: 6000\n",
      "Scanned: 7000\n",
      "Scanned: 8000\n",
      "Scanned: 9000\n",
      "Scanned: 10000\n",
      "Scanned: 11000\n",
      "Scanned: 12000\n",
      "Scanned: 13000\n",
      "Scanned: 14000\n",
      "Scanned: 15000\n",
      "Scanned: 16000\n",
      "Scanned: 17000\n",
      "Scanned: 18000\n",
      "Scanned: 19000\n",
      "Scanned: 20000\n",
      "Scanned: 21000\n",
      "Scanned: 22000\n",
      "Scanned: 23000\n",
      "Scanned: 24000\n",
      "Scanned: 25000\n",
      "Scanned: 26000\n",
      "Scanned: 27000\n",
      "Scanned: 28000\n",
      "Scanned: 29000\n",
      "Scanned: 30000\n",
      "Scanned: 31000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@param datasets_directory_path path to the directory where there are all the datasets\n",
    "@param checker_error_log_path path of the log file of the dataset checker\n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postDownloadStats():\n",
    "\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "    f_log_checker = open(checker_error_log_path, \"r\")\n",
    "\n",
    "    n_datasets = 0\n",
    "    n_empty = 0\n",
    "    n_full = 0\n",
    "    n_links = 0\n",
    "    success_links = 0\n",
    "    error_links = 0 \n",
    "\n",
    "    #scan the datasets folders and extract download info\n",
    "\n",
    "    for folder in os.scandir(datasets_directory_path):\n",
    "        \n",
    "        n_datasets+=1\n",
    "\n",
    "        if n_datasets % 1000 == 0:\n",
    "            print(\"Scanned: \"+str(n_datasets))\n",
    "\n",
    "        dataset_json_path = datasets_directory_path+\"/\"+folder.name+\"/dataset_metadata.json\"\n",
    "\n",
    "        #open the dataset.json file \n",
    "        dataset_json_file=open(dataset_json_path, \"r\")\n",
    "        \n",
    "        #load the json object present in the json datasets list\n",
    "        dataset_json = json.load(dataset_json_file,strict=False)\n",
    "\n",
    "        if dataset_json[\"download_info\"][\"downloaded\"] == 0:\n",
    "            n_empty+=1 \n",
    "        elif dataset_json[\"download_info\"][\"downloaded\"] == dataset_json[\"download_info\"][\"total_URLS\"]:\n",
    "            n_full+=1\n",
    "\n",
    "        n_links += dataset_json[\"download_info\"][\"total_URLS\"]\n",
    "        success_links += dataset_json[\"download_info\"][\"downloaded\"]\n",
    "        error_links += (dataset_json[\"download_info\"][\"total_URLS\"] - dataset_json[\"download_info\"][\"downloaded\"])\n",
    "        \n",
    "        dataset_json_file.close()\n",
    "        del(dataset_json)\n",
    "        \n",
    "    #read the checker error_log\n",
    "\n",
    "    n_file = 0\n",
    "        \n",
    "    while True:\n",
    "\n",
    "        line = f_log_checker.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        #split the line\n",
    "        fields = line.split(\": \")\n",
    "\n",
    "        if fields[0] == \"File\":\n",
    "            n_file += 1\n",
    "\n",
    "    output_file.write(\"Number of tried links:\"+str(n_links)+\"\\n\")\n",
    "    output_file.write(\"Number of success links:\"+str(success_links)+\"\\n\")\n",
    "    output_file.write(\"Number of error links:\"+str(error_links)+\"\\n\")\n",
    "    output_file.write(\"Number of datasets: \"+str(n_datasets)+\"\\n\")\n",
    "    output_file.write(\"Number of full datasets: \"+str(n_full)+\"\\n\")\n",
    "    output_file.write(\"Number of partial datasets: \"+str(n_datasets-n_full-n_empty)+\"\\n\")\n",
    "    output_file.write(\"Number of empty datasets: \"+str(n_empty)+\"\\n\")\n",
    "    output_file.write(\"Number of files that need to be assigned to an extension: \"+str(n_file)+\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    f_log_checker.close()\n",
    "\n",
    "dirname = os.path.dirname(__name__)\n",
    "logs_path = os.path.join(dirname, '../download/logs/downloader_errors.log')\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                               #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join('output/post_download_statistics.txt')                                #path to the output file\n",
    "\n",
    "checker_error_log_path = os.path.join(dirname, '../../download/logs/check_datasets.log')              #path to the check_datasets.py log\n",
    "postDownloadStats() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
