{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Recovering\n",
    "In this notebook we are going to retrieve some collection stats after the file  phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import tqdm \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "SUFFIXES = [\".rdf\", \".rdfs\", \".ttl\", \".owl\", \".n3\", \".nt\", \".jsonld\", \".xml\", \".ntriples\", \".nq\", \".trig\", \".trix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic File Recovering\n",
    "\n",
    "After the automatic file recovering we want to know: \n",
    "* how many files have been recovered\n",
    "* how many files need to be manually processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@param recover_error_log_path path to the file recover error log path \n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postAutomaticRecovering():\n",
    "    f_log_recover = open(recover_error_log_path, \"r\")\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "\n",
    "    #read the recover error_log\n",
    "\n",
    "    n_unrecovered_file = 0\n",
    "    n_recovered_file = 0\n",
    "        \n",
    "    while True:\n",
    "\n",
    "        line = f_log_recover.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        #split the line\n",
    "        fields = line.split(\": \")\n",
    "\n",
    "        if fields[0] == \"Dataset\":\n",
    "            n_unrecovered_file += 1\n",
    "        elif fields[0] == \"Recover in Dataset\": \n",
    "            n_recovered_file += 1\n",
    "\n",
    "    output_file.write(\"Number of recovered files: \"+str(n_recovered_file)+\"\\n\")\n",
    "    output_file.write(\"Number of unrecovered files: \"+str(n_unrecovered_file)+\"\\n\")\n",
    "\n",
    "    f_log_recover.close()\n",
    "    output_file.close()\n",
    "\n",
    "dirname = os.path.dirname(__name__)\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                         #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(dirname, 'output/post_automatic_recovering_statistics.txt')     #path to the error output file\n",
    "\n",
    "recover_error_log_path = os.path.join(dirname, '../../download/logs/check_datasets.log')        #path to the check_datasets error log file\n",
    "postAutomaticRecovering() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual File Recovering\n",
    "\n",
    "After the manual file recovering we want to know: \n",
    "* total number of files in the collection\n",
    "* total number of files with a no RDF extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned: 1000\n",
      "Scanned: 2000\n",
      "Scanned: 3000\n",
      "Scanned: 4000\n",
      "Scanned: 5000\n",
      "Scanned: 6000\n",
      "Scanned: 7000\n",
      "Scanned: 8000\n",
      "Scanned: 9000\n",
      "Scanned: 10000\n",
      "Scanned: 11000\n",
      "Scanned: 12000\n",
      "Scanned: 13000\n",
      "Scanned: 14000\n",
      "Scanned: 15000\n",
      "Scanned: 16000\n",
      "Scanned: 17000\n",
      "Scanned: 18000\n",
      "Scanned: 19000\n",
      "Scanned: 20000\n",
      "Scanned: 21000\n",
      "Scanned: 22000\n",
      "Scanned: 23000\n",
      "Scanned: 24000\n",
      "Scanned: 25000\n",
      "Scanned: 26000\n",
      "Scanned: 27000\n",
      "Scanned: 28000\n",
      "Scanned: 29000\n",
      "Scanned: 30000\n",
      "Scanned: 31000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postManualRecovering():\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "\n",
    "    nFiles = 0\n",
    "    nNoRDFFiles = 0\n",
    "    nDataset = 0\n",
    "\n",
    "    for dataset in os.scandir(datasets_directory_path):\n",
    "        for file in os.scandir(dataset):\n",
    "            if file.name != \"dataset_metadata.json\" and file.name != \"dataset_content.json\": \n",
    "                file_suffix = pathlib.Path(file.path).suffix\n",
    "                if file_suffix not in SUFFIXES:\n",
    "                    nNoRDFFiles += 1\n",
    "                nFiles+=1\n",
    "        nDataset+=1\n",
    "        if nDataset%1000 == 0:\n",
    "            print(\"Scanned: \"+str(nDataset))\n",
    "\n",
    "    output_file.write(\"Total number of files: \"+str(nFiles)+\"\\n\")\n",
    "    output_file.write(\"Total number of NO RDF files: \"+str(nNoRDFFiles)+\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "dirname = os.path.dirname(__name__)\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                     #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(dirname, 'output/post_manual_recovering_statistics.txt')    #path to the output file\n",
    "\n",
    "postManualRecovering() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
