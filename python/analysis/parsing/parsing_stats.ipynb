{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "In this notebook we are going to retrieve some collection stats after the parsing phase.\n",
    "\n",
    "We are actually testing 3 parsing strategies:\n",
    "* Standard Parsing (Jena + RDFLib + LightRDF)\n",
    "* Labels v1 Parsing (only RDFLib)\n",
    "* Labels v2 Parsing (only RDFLib)\n",
    "\n",
    "Labels v1 and Labels v2 are actually using the same parser (RDFLib) so we are going to consider them as one unique parsing strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import tqdm \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "SUFFIXES = [\".rdf\", \".rdfs\", \".ttl\", \".owl\", \".n3\", \".nt\", \".jsonld\", \".xml\", \".ntriples\", \".nq\", \".trig\", \".trix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After the standard parsing phase we want to know: \n",
    "* number of files parsed by JENA\n",
    "* number of files parsed by RDFLib\n",
    "* number of files parsed by LightRDF\n",
    "* number of correctly parsed files\n",
    "* number of not parsed files\n",
    "* how many datasets are full, partial and empty\n",
    "\n",
    "We will also output a json file with the empty datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked: 0\n",
      "Checked: 1000\n",
      "Checked: 2000\n",
      "Checked: 3000\n",
      "Checked: 4000\n",
      "Checked: 5000\n",
      "Checked: 6000\n",
      "Checked: 7000\n",
      "Checked: 8000\n",
      "Checked: 9000\n",
      "Checked: 10000\n",
      "Checked: 11000\n",
      "Checked: 12000\n",
      "Checked: 13000\n",
      "Checked: 14000\n",
      "Checked: 15000\n",
      "Checked: 16000\n",
      "Checked: 17000\n",
      "Checked: 18000\n",
      "Checked: 19000\n",
      "Checked: 20000\n",
      "Checked: 21000\n",
      "Checked: 22000\n",
      "Checked: 23000\n",
      "Checked: 24000\n",
      "Checked: 25000\n",
      "Checked: 26000\n",
      "Checked: 27000\n",
      "Checked: 28000\n",
      "Checked: 29000\n",
      "Checked: 30000\n",
      "Checked: 31000\n"
     ]
    }
   ],
   "source": [
    "# from the post manual file recovering \n",
    "N_FILES = 28537        #total number of files in the collection\n",
    "N_NOT_RDF_FILES = 61   #number of not RDF files\n",
    "\n",
    "# constants for the dataset type\n",
    "EMPTY = 0\n",
    "PARTIAL = 1\n",
    "FULL = 2\n",
    "\n",
    "def postParsing():\n",
    "    output_file = open(\"output/post_standard_parsing_stats.txt\", \"a\")\n",
    "    jena_error_file = open(jena_error_log_path, \"r\")\n",
    "    rdflib_error_file = open(rdflib_error_log_path, \"r\")\n",
    "\n",
    "    #count the number of files not parsable by jena  and rdflib\n",
    "    notParsableJena = 0\n",
    "\n",
    "    while True:\n",
    "        line = jena_error_file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        if \"Error: \" in line:\n",
    "            notParsableJena+=1\n",
    "\n",
    "    notParsableRDFLib = 0\n",
    "\n",
    "    while True:\n",
    "        line = rdflib_error_file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        if \"Error: \" in line:\n",
    "            notParsableRDFLib+=1\n",
    "    \n",
    "    jena_error_file.close()\n",
    "    rdflib_error_file.close()\n",
    "\n",
    "    full_datasets = 0 \n",
    "    full_datasets_list = list()\n",
    "    partial_datasets = 0 \n",
    "    partial_datasets_list = list()\n",
    "    empty_datasets = 0 \n",
    "    empty_datasets_list = list()\n",
    "    parsable_jena = 0\n",
    "    parsable_rdflib = 0\n",
    "    parsable_lightrdf = 0\n",
    "\n",
    "    datasets = 0 \n",
    "    total_parsed_files = 0\n",
    "\n",
    "    for dataset in os.scandir(datasets_directory_path):\n",
    "\n",
    "        if datasets % 1000 == 0:\n",
    "            print(\"Checked: \"+str(datasets))\n",
    "\n",
    "        #open the dataset metadata file\n",
    "        dataset_metadata_file = open(dataset.path+\"/dataset_metadata.json\", \"r\", encoding=\"utf-8\")\n",
    "        dataset_metadata = json.load(dataset_metadata_file, strict = False)\n",
    "        dataset_metadata_file.close()\n",
    "\n",
    "        #check if the dataset is empty, partial or full based on the download info\n",
    "        if dataset_metadata[\"download_info\"][\"downloaded\"] == dataset_metadata[\"download_info\"][\"total_URLS\"]: \n",
    "            dataset_download_type = FULL\n",
    "        elif dataset_metadata[\"download_info\"][\"downloaded\"] == 0:\n",
    "            dataset_download_type = EMPTY\n",
    "        else:\n",
    "            dataset_download_type = PARTIAL\n",
    "    \n",
    "        if dataset_download_type == EMPTY:\n",
    "            empty_datasets += 1\n",
    "            empty_datasets_list.append(dataset.name)\n",
    "        else:\n",
    "\n",
    "            #count how many rdf files\n",
    "            n_rdf_files = 0\n",
    "            for file in os.scandir(dataset):\n",
    "                if pathlib.Path(file.path).suffix in SUFFIXES:\n",
    "                    n_rdf_files+=1\n",
    "            \n",
    "            parsed_files = 0\n",
    "\n",
    "            if \"mined_files_jena\" in dataset_metadata:\n",
    "                parsed_files += len(dataset_metadata[\"mined_files_jena\"])\n",
    "                parsable_jena += len(dataset_metadata[\"mined_files_jena\"])\n",
    "            if \"mined_files_rdflib\" in dataset_metadata:\n",
    "                parsed_files += len(dataset_metadata[\"mined_files_rdflib\"])\n",
    "                parsable_rdflib += len(dataset_metadata[\"mined_files_rdflib\"])\n",
    "            if \"mined_files_lightrdf\" in dataset_metadata:\n",
    "                parsed_files += len(dataset_metadata[\"mined_files_lightrdf\"])\n",
    "                parsable_lightrdf += len(dataset_metadata[\"mined_files_lightrdf\"])\n",
    "\n",
    "            if parsed_files == 0:\n",
    "                empty_datasets += 1\n",
    "                empty_datasets_list.append(dataset.name)\n",
    "            \n",
    "            if parsed_files == n_rdf_files:\n",
    "                if dataset_download_type == FULL:\n",
    "                    full_datasets += 1\n",
    "                    full_datasets_list.append(dataset.name)\n",
    "                elif dataset_download_type == PARTIAL:\n",
    "                    partial_datasets += 1\n",
    "                    partial_datasets_list.append(dataset.name)\n",
    "            elif parsed_files > 0:\n",
    "                partial_datasets += 1\n",
    "                partial_datasets_list.append(dataset.name)\n",
    "            \n",
    "            total_parsed_files += parsed_files\n",
    "\n",
    "        datasets+=1\n",
    "\n",
    "    output_file.write(\"Total number of parsable files for Jena: \"+str(parsable_jena)+\"\\n\")\n",
    "    output_file.write(\"Increment of parsable files for RDFLib: \"+str(parsable_rdflib)+\"\\n\")\n",
    "    output_file.write(\"Increment of parsable files for LightRDF: \"+str(parsable_lightrdf)+\"\\n\")\n",
    "    output_file.write(\"Total number of parsed files: \"+str(total_parsed_files)+\"\\n\" )\n",
    "    output_file.write(\"Total number of not parsable files: \"+str(N_FILES - total_parsed_files)+\"\\n\" )\n",
    "    output_file.write(\"Total number of full datasets: \"+str(full_datasets)+\"\\n\" )\n",
    "    output_file.write(\"Total number of partial datasets: \"+str(partial_datasets)+\"\\n\" )\n",
    "    output_file.write(\"Total number of empty datasets: \"+str(empty_datasets)+\"\\n\" )\n",
    "    output_file.close()\n",
    "\n",
    "    #save the empty datasets in a json file in the output folder\n",
    "    with open('output/lists/standard/empty_datasets_standard.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(empty_datasets_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open('output/lists/standard/partial_datasets_standard.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(partial_datasets_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open('output/lists/standard/full_datasets_standard.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(full_datasets_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "dirname = os.path.dirname(__name__)\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                           #path to the folder of the downloaded datasets\n",
    "jena_error_log_path = os.path.join(dirname, '../../mining/logs/jena_miner_error.log')             #path to the jena error log file\n",
    "rdflib_error_log_path = os.path.join(dirname, '../../mining/logs/rdflib_miner_error.log')         #path to the rdflib error log file\n",
    "lightrdf_error_log_path = os.path.join(dirname, '../../mining/logs/lightrdf_miner_error.log')     #path to the lightrdf error log file\n",
    "\n",
    "\n",
    "postParsing() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Parsing v1 and v2\n",
    "\n",
    "Since for the 2 parsing versions we are using RDFLib in the same manner the stats will be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked: 0\n",
      "Checked: 1000\n",
      "Checked: 2000\n",
      "Checked: 3000\n",
      "Checked: 4000\n",
      "Checked: 5000\n",
      "Checked: 6000\n",
      "Checked: 7000\n",
      "Checked: 8000\n",
      "Checked: 9000\n",
      "Checked: 10000\n",
      "Checked: 11000\n",
      "Checked: 12000\n",
      "Checked: 13000\n",
      "Checked: 14000\n",
      "Checked: 15000\n",
      "Checked: 16000\n",
      "Checked: 17000\n",
      "Checked: 18000\n",
      "Checked: 19000\n",
      "Checked: 20000\n",
      "Checked: 21000\n",
      "Checked: 22000\n",
      "Checked: 23000\n",
      "Checked: 24000\n",
      "Checked: 25000\n",
      "Checked: 26000\n",
      "Checked: 27000\n",
      "Checked: 28000\n",
      "Checked: 29000\n",
      "Checked: 30000\n",
      "Checked: 31000\n"
     ]
    }
   ],
   "source": [
    "def postHRParsing():\n",
    "    output_file = open(\"output/post_parsing_labels_stats.txt\", \"a\")\n",
    "    rdflibhr_error_file = open(rdflibhr_error_log_path, \"r\")\n",
    "\n",
    "    notParsableRDFLib = 0\n",
    "\n",
    "    while True:\n",
    "        line = rdflibhr_error_file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        if \"Error: \" in line:\n",
    "            notParsableRDFLib+=1\n",
    "\n",
    "    rdflibhr_error_file.close()\n",
    "\n",
    "    datasets = 0 \n",
    "    empty_datasets = 0\n",
    "    empty_datasets_list = list()\n",
    "    partial_datasets = 0\n",
    "    partial_datasets_list = list()\n",
    "    full_datasets = 0\n",
    "    full_datasets_list = list()\n",
    "    \n",
    "    total_parsed_files = 0\n",
    "\n",
    "    for dataset in os.scandir(datasets_directory_path):\n",
    "\n",
    "        if datasets % 1000 == 0:\n",
    "            print(\"Checked: \"+str(datasets))\n",
    "\n",
    "        #open the dataset metadata file\n",
    "        dataset_metadata_file = open(dataset.path+\"/dataset_metadata.json\", \"r\", encoding=\"utf-8\")\n",
    "        dataset_metadata = json.load(dataset_metadata_file, strict = False)\n",
    "        dataset_metadata_file.close()\n",
    "\n",
    "        dataset_download_type = EMPTY\n",
    "        if len(dataset_metadata[\"failed_download_urls\"]) == 0:\n",
    "            dataset_download_type = FULL\n",
    "        elif len(dataset_metadata[\"failed_download_urls\"]) > 0 and len(dataset_metadata[\"downloaded_urls\"]) > 0:\n",
    "            dataset_download_type = PARTIAL\n",
    "        else: \n",
    "            dataset_download_type = EMPTY\n",
    "\n",
    "        parsed_files = 0\n",
    "\n",
    "        if \"mined_files_rdflibhr\" in dataset_metadata:\n",
    "            parsed_files = len(dataset_metadata[\"mined_files_rdflibhr\"])\n",
    "            total_parsed_files += parsed_files\n",
    "\n",
    "        if parsed_files == 0:\n",
    "            empty_datasets += 1\n",
    "            empty_datasets_list.append(dataset.name)\n",
    "\n",
    "        else:    \n",
    "            total_files_set = set()\n",
    "            for file in os.scandir(dataset):\n",
    "                if pathlib.Path(file.path).suffix in SUFFIXES: \n",
    "                    total_files_set.add(file.name.split(\".\")[0])\n",
    "\n",
    "            if parsed_files == len(total_files_set):\n",
    "                if dataset_download_type == FULL:\n",
    "                    full_datasets += 1\n",
    "                    full_datasets_list.append(dataset.name)\n",
    "                elif dataset_download_type == PARTIAL:\n",
    "                    partial_datasets += 1\n",
    "                    partial_datasets_list.append(dataset.name)\n",
    "\n",
    "            elif parsed_files > 0:\n",
    "                partial_datasets += 1\n",
    "                partial_datasets_list.append(dataset.name)\n",
    "                \n",
    "        datasets += 1\n",
    "\n",
    "    output_file.write(\"Total number of parsed files by RDFLibHR: \"+ str(total_parsed_files)+\"\\n\")\n",
    "    output_file.write(\"Number of full datasets: \"+str(full_datasets)+\"\\n\")\n",
    "    output_file.write(\"Number of partial datasets: \"+str(partial_datasets)+\"\\n\")\n",
    "    output_file.write(\"Number of empty datasets: \"+str(empty_datasets)+\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "    #save the empty datasets in a json file in the output folder\n",
    "    with open('output/lists/labels/empty_datasets_standard.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(empty_datasets_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open('output/lists/labels/partial_datasets_standard.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(partial_datasets_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open('output/lists/labels/full_datasets_standard.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(full_datasets_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "dirname = os.path.dirname(__name__)\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                            #path to the folder of the downloaded datasets\n",
    "rdflibhr_error_log_path = os.path.join(dirname, '../../mining/logs/rdflibhr_miner_error.log')      #path to the rdflib error log file\n",
    "\n",
    "\n",
    "postHRParsing() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
