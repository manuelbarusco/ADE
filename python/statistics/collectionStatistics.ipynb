{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection Statistics\n",
    "\n",
    "This notebook will calculate some collection statistic during the various phases of the processing. In particular: \n",
    "* after download phase (and download retry)\n",
    "* after automatic file recovering phase\n",
    "* after manual file recovering phase\n",
    "* after indexing phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import tqdm \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download\n",
    "\n",
    "After collection download we want to know:\n",
    "* how many datasets are complete (all the files for the dataset are downloaded)\n",
    "* how many datasets are partial (not all the files for the dataset are downloaded)\n",
    "* how many datasets are empty (no files for the dataset are downloaded)\n",
    "* how many files have not a valid RDF extension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned: 1000\n",
      "Scanned: 2000\n",
      "Scanned: 3000\n",
      "Scanned: 4000\n",
      "Scanned: 5000\n",
      "Scanned: 6000\n",
      "Scanned: 7000\n",
      "Scanned: 8000\n",
      "Scanned: 9000\n",
      "Scanned: 10000\n",
      "Scanned: 11000\n",
      "Scanned: 12000\n",
      "Scanned: 13000\n",
      "Scanned: 14000\n",
      "Scanned: 15000\n",
      "Scanned: 16000\n",
      "Scanned: 17000\n",
      "Scanned: 18000\n",
      "Scanned: 19000\n",
      "Scanned: 20000\n",
      "Scanned: 21000\n",
      "Scanned: 22000\n",
      "Scanned: 23000\n",
      "Scanned: 24000\n",
      "Scanned: 25000\n",
      "Scanned: 26000\n",
      "Scanned: 27000\n",
      "Scanned: 28000\n",
      "Scanned: 29000\n",
      "Scanned: 30000\n",
      "Scanned: 31000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@param datasets_directory_path path to the directory where there are all the datasets\n",
    "@param checker_error_log_path path of the log file of the dataset checker\n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postDownloadStats():\n",
    "\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "    f_log_checker = open(checker_error_log_path, \"r\")\n",
    "\n",
    "    n_datasets = 0\n",
    "    n_empty = 0\n",
    "    n_full = 0\n",
    "\n",
    "    #scan the datasets folders and count how many datasets are full, partial and empty\n",
    "\n",
    "    for folder in os.scandir(datasets_directory_path):\n",
    "        \n",
    "        n_datasets+=1\n",
    "\n",
    "        if n_datasets % 1000 == 0:\n",
    "            print(\"Scanned: \"+str(n_datasets))\n",
    "\n",
    "        dataset_json_path = datasets_directory_path+\"/\"+folder.name+\"/dataset_metadata.json\"\n",
    "\n",
    "        #open the dataset.json file \n",
    "        dataset_json_file=open(dataset_json_path, \"r\")\n",
    "        \n",
    "        #load the json object present in the json datasets list\n",
    "        dataset_json = json.load(dataset_json_file,strict=False)\n",
    "\n",
    "        if dataset_json[\"download_info\"][\"downloaded\"] == 0:\n",
    "            n_empty+=1 \n",
    "        \n",
    "        if dataset_json[\"download_info\"][\"downloaded\"] == dataset_json[\"download_info\"][\"total_URLS\"]:\n",
    "            n_full+=1\n",
    "        \n",
    "        dataset_json_file.close()\n",
    "        del(dataset_json)\n",
    "        \n",
    "    #read the checker error_log\n",
    "\n",
    "    n_file = 0\n",
    "        \n",
    "    while True:\n",
    "\n",
    "        line = f_log_checker.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        #split the line\n",
    "        fields = line.split(\": \")\n",
    "\n",
    "        if fields[0] == \"File\":\n",
    "            n_file += 1\n",
    "\n",
    "    output_file.write(\"Number of datasets: \"+str(n_datasets)+\"\\n\")\n",
    "    output_file.write(\"Number of full datasets: \"+str(n_full)+\"\\n\")\n",
    "    output_file.write(\"Number of partial datasets: \"+str(n_datasets-n_full-n_empty)+\"\\n\")\n",
    "    output_file.write(\"Number of empty datasets: \"+str(n_empty)+\"\\n\")\n",
    "    output_file.write(\"Number of files that need to be assigned to an extension: \"+str(n_file)+\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    f_log_checker.close()\n",
    "\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                               #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, '../../logs/statistics/post_download_statistics.txt')      #path to the error log file\n",
    "\n",
    "checker_error_log_path = os.path.join(scriptDir, '../../logs/checker_error_log.txt')                  #path to the checker error log file\n",
    "postDownloadStats() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic File Recovering\n",
    "\n",
    "After the automatic file recovering we want to know: \n",
    "* how many files have been recovered\n",
    "* how many files need to be manually processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@param recover_error_log_path path to the file recover error log path \n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postAutomaticRecovering():\n",
    "    f_log_recover = open(recover_error_log_path, \"r\")\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "\n",
    "    #read the recover error_log\n",
    "\n",
    "    n_unrecovered_file = 0\n",
    "    n_recovered_file = 0\n",
    "        \n",
    "    while True:\n",
    "\n",
    "        line = f_log_recover.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        #split the line\n",
    "        fields = line.split(\": \")\n",
    "\n",
    "        if fields[0] == \"Dataset\":\n",
    "            n_unrecovered_file += 1\n",
    "        elif fields[0] == \"Recover in Dataset\": \n",
    "            n_recovered_file += 1\n",
    "\n",
    "    output_file.write(\"Number of recovered files: \"+str(n_recovered_file)+\"\\n\")\n",
    "    output_file.write(\"Number of unrecovered files: \"+str(n_unrecovered_file)+\"\\n\")\n",
    "\n",
    "    f_log_recover.close()\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, '../../logs/statistics/post_automatic_recovering_statistics.txt')     #path to the error log file\n",
    "\n",
    "recover_error_log_path = os.path.join(scriptDir, '../../logs/recover_error_log.txt')                  #path to the checker error log file\n",
    "postAutomaticRecovering() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual File Recovering\n",
    "\n",
    "After the manual file recovering we want to know: \n",
    "* total number of files in the collection\n",
    "* total number of files with a no RDF extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned: 1000\n",
      "Scanned: 2000\n",
      "Scanned: 3000\n",
      "Scanned: 4000\n",
      "Scanned: 5000\n",
      "Scanned: 6000\n",
      "Scanned: 7000\n",
      "Scanned: 8000\n",
      "Scanned: 9000\n",
      "Scanned: 10000\n",
      "Scanned: 11000\n",
      "Scanned: 12000\n",
      "Scanned: 13000\n",
      "Scanned: 14000\n",
      "Scanned: 15000\n",
      "Scanned: 16000\n",
      "Scanned: 17000\n",
      "Scanned: 18000\n",
      "Scanned: 19000\n",
      "Scanned: 20000\n",
      "Scanned: 21000\n",
      "Scanned: 22000\n",
      "Scanned: 23000\n",
      "Scanned: 24000\n",
      "Scanned: 25000\n",
      "Scanned: 26000\n",
      "Scanned: 27000\n",
      "Scanned: 28000\n",
      "Scanned: 29000\n",
      "Scanned: 30000\n",
      "Scanned: 31000\n"
     ]
    }
   ],
   "source": [
    "SUFFIXES = [\".rdf\", \".rdfs\", \".ttl\", \".owl\", \".n3\", \".nt\", \".jsonld\", \".xml\", \".ntriples\", \".nq\", \".trig\", \".trix\"]\n",
    "\n",
    "'''\n",
    "@param output_file_path file where to write the output statistics\n",
    "'''\n",
    "def postManualRecovering():\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "\n",
    "    nFiles = 0\n",
    "    nNoRDFFiles = 0\n",
    "    nDataset = 0\n",
    "\n",
    "    for dataset in os.scandir(datasets_directory_path):\n",
    "        for file in os.scandir(dataset):\n",
    "            if file.name != \"dataset_metadata.json\" and file.name != \"dataset_content.json\": \n",
    "                file_suffix = pathlib.Path(file.path).suffix\n",
    "                if file_suffix not in SUFFIXES:\n",
    "                    nNoRDFFiles += 1\n",
    "                nFiles+=1\n",
    "        nDataset+=1\n",
    "        if nDataset%1000 == 0:\n",
    "            print(\"Scanned: \"+str(nDataset))\n",
    "\n",
    "    output_file.write(\"Total number of files: \"+str(nFiles)+\"\\n\")\n",
    "    output_file.write(\"Total number of NO RDF files: \"+str(nNoRDFFiles)+\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                #path to the folder of the downloaded datasets\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, '../../logs/statistics/post_manual_recovering_statistics.txt')     #path to the error log file\n",
    "\n",
    "postManualRecovering() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated Datasets\n",
    "\n",
    "In this section we are going to analyze which datasets are duplicated. A dataset is considered duplicated if there is another dataset with the same link. Then we are going to consider which are the differences (if there are any) in the datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated: 438 datasets\n"
     ]
    }
   ],
   "source": [
    "def findDuplicates(datasets_list_path, output_file_path):\n",
    "\n",
    "    #open the json file with the datasets list\n",
    "    dataset_list_file=open(datasets_list_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    #open the output file\n",
    "    output_file=open(output_file_path, \"a\")\n",
    "\n",
    "    #load the json list\n",
    "    datasets_list = json.load(dataset_list_file,strict=False)\n",
    "\n",
    "    #create a dictionary of key: link, value: list of dataset ids that download that link\n",
    "    links = dict()\n",
    "\n",
    "    for entry in datasets_list[\"datasets\"]:\n",
    "        dataset_id = entry[\"dataset_id\"]\n",
    "\n",
    "        distinct_links = sorted(set(entry[\"download\"]))\n",
    "\n",
    "        links_as_string = \"\"\n",
    "        for link in distinct_links:\n",
    "            links_as_string += link\n",
    "\n",
    "        if links_as_string not in links.keys():\n",
    "            links[links_as_string] = list()\n",
    "\n",
    "        links[links_as_string].append(dataset_id)\n",
    "\n",
    "    i = 0 \n",
    "    for links_as_string, datasets in links.items():\n",
    "        if len(datasets) > 1:\n",
    "            output_file.write(str(datasets)+\"\\n\")\n",
    "            i+=1\n",
    "    print(\"Duplicated: \"+str(i)+\" datasets\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "datasets_list_path = \"/home/manuel/Tesi/ACORDAR/Data/datasets.json\"\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, '../../logs/statistics/duplicated_datasets.txt')      #path to the jena error log file\n",
    "\n",
    "\n",
    "findDuplicates(datasets_list_path, output_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "\n",
    "After the parsing phase we want to know: \n",
    "* total number of not parsable files for JENA / RDFLib\n",
    "* total number of parsable files for JENA / RDFLib\n",
    "* totale number of complete, partial and empty datasets for JENA / RDFLib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked: 0\n",
      "Checked: 1000\n",
      "Checked: 2000\n",
      "Checked: 3000\n",
      "Checked: 4000\n",
      "Checked: 5000\n",
      "Checked: 6000\n",
      "Checked: 7000\n",
      "Checked: 8000\n",
      "Checked: 9000\n",
      "Checked: 10000\n",
      "Checked: 11000\n",
      "Checked: 12000\n",
      "Checked: 13000\n",
      "Checked: 14000\n",
      "Checked: 15000\n",
      "Checked: 16000\n",
      "Checked: 17000\n",
      "Checked: 18000\n",
      "Checked: 19000\n",
      "Checked: 20000\n",
      "Checked: 21000\n",
      "Checked: 22000\n",
      "Checked: 23000\n",
      "Checked: 24000\n",
      "Checked: 25000\n",
      "Checked: 26000\n",
      "Checked: 27000\n",
      "Checked: 28000\n",
      "Checked: 29000\n",
      "Checked: 30000\n",
      "Checked: 31000\n"
     ]
    }
   ],
   "source": [
    "N_FILES = 33131        #total number of files in the collection\n",
    "N_NOT_RDF_FILES = 142  #number of not RDF files\n",
    "\n",
    "def postParsing():\n",
    "    output_file = open(output_file_path, \"a\")\n",
    "    jena_error_file = open(jena_error_log_path, \"r\")\n",
    "    rdflib_error_file = open(rdflib_error_log_path, \"r\")\n",
    "\n",
    "    #count the number of files not parsable for jena  and rdflib\n",
    "    notParsableJena = 0\n",
    "\n",
    "    while True:\n",
    "        line = jena_error_file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        if \"Error: \" in line:\n",
    "            notParsableJena+=1\n",
    "\n",
    "    notParsableRDFLib = 0\n",
    "\n",
    "    while True:\n",
    "        line = rdflib_error_file.readline()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        if \"Error: \" in line:\n",
    "            notParsableRDFLib+=1\n",
    "    \n",
    "    jena_error_file.close()\n",
    "    rdflib_error_file.close()\n",
    "\n",
    "    full_datasets_jena = 0\n",
    "    empty_datasets_jena = 0\n",
    "    partial_datasets_jena = 0\n",
    "\n",
    "    full_datasets_rdflib = 0\n",
    "    empty_datasets_rdflib = 0\n",
    "    partial_datasets_rdflib = 0\n",
    "\n",
    "    datasets = 0 \n",
    "\n",
    "    for dataset in os.scandir(datasets_directory_path):\n",
    "\n",
    "        if datasets % 1000 == 0:\n",
    "            print(\"Checked: \"+str(datasets))\n",
    "\n",
    "        #open the dataset metadata file\n",
    "        dataset_metadata_file = open(dataset.path+\"/dataset_metadata.json\", \"r\", encoding=\"utf-8\")\n",
    "        dataset_metadata = json.load(dataset_metadata_file, strict = False)\n",
    "        dataset_metadata_file.close()\n",
    "\n",
    "        nfiles = 0\n",
    "        if \"mined_files_rdflib\" in  dataset_metadata:\n",
    "            nfiles = len(os.listdir(dataset)) - 3\n",
    "        else:\n",
    "            nfiles = len(os.listdir(dataset)) - 2\n",
    "        \n",
    "        #count for jena\n",
    "        if dataset_metadata[\"mined_files_jena\"] == nfiles:\n",
    "            full_datasets_jena += 1\n",
    "        elif dataset_metadata[\"mined_files_jena\"] > 0 and dataset_metadata[\"mined_files_jena\"] < nfiles:\n",
    "            partial_datasets_jena += 1\n",
    "        elif dataset_metadata[\"mined_files_jena\"] == 0:\n",
    "            empty_datasets_jena += 1\n",
    "\n",
    "        #count for rdflib\n",
    "        if \"mined_files_rdflib\" in  dataset_metadata:\n",
    "            minedFiles = dataset_metadata[\"mined_files_jena\"] + dataset_metadata[\"mined_files_rdflib\"]  \n",
    "            if minedFiles == nfiles:\n",
    "                full_datasets_rdflib += 1\n",
    "            elif minedFiles > 0 and minedFiles < nfiles:\n",
    "                partial_datasets_rdflib += 1\n",
    "            elif minedFiles == 0:\n",
    "                empty_datasets_rdflib += 1\n",
    "        \n",
    "        datasets+=1\n",
    "\n",
    "\n",
    "    output_file.write(\"Total number of parsable files for Jena: \"+str(N_FILES-notParsableJena)+\"\\n\")\n",
    "    output_file.write(\"Total number of not parsable files for Jena: \"+str(notParsableJena)+\"\\n\")\n",
    "    output_file.write(\"Total number of parsable files for RDFLib: \"+str(N_FILES-notParsableRDFLib)+\"\\n\")\n",
    "    output_file.write(\"Total number of not parsable files for RDFLib: \"+str(notParsableRDFLib)+\"\\n\")\n",
    "    output_file.write(\"Total number of full datasets for Jena: \"+str(full_datasets_jena)+\"\\n\" )\n",
    "    output_file.write(\"Total number of partial datasets for Jena: \"+str(partial_datasets_jena)+\"\\n\" )\n",
    "    output_file.write(\"Total number of empty datasets for Jena: \"+str(empty_datasets_jena)+\"\\n\" )\n",
    "    output_file.write(\"Incremental of full datasets for RDFLib: \"+str(full_datasets_rdflib)+\"\\n\" )\n",
    "    output_file.write(\"Incremental of partial datasets for RDFLib: \"+str(partial_datasets_rdflib)+\"\\n\" )\n",
    "    output_file.write(\"Total number of empty datasets for RDFLib: \"+str(empty_datasets_rdflib)+\"\\n\" )\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "scriptDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "datasets_directory_path = \"/media/manuel/Tesi/Datasets\"                                 #path to the folder of the downloaded datasets\n",
    "jena_error_log_path = os.path.join(scriptDir, '../../logs/jena_miner_error_log.txt')      #path to the jena error log file\n",
    "rdflib_error_log_path = os.path.join(scriptDir, '../../logs/rdflib_miner_error_log.txt')      #path to the rdflib error log file\n",
    "\n",
    "output_file_path = os.path.join(scriptDir, '../../logs/statistics/post_parsing_statistics.txt')     #path to the error log file\n",
    "\n",
    "postParsing() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
